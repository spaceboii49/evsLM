{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-10T05:27:49.160912Z",
     "start_time": "2025-09-10T05:27:49.105626Z"
    }
   },
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import re\n",
    "import os\n",
    "import glob"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T05:30:25.588848Z",
     "start_time": "2025-09-10T05:28:44.547965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    #replacing multiple spaces and newlines with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    #fixing hyphenation issues\n",
    "    text = re.sub(r'-\\s+', '', text)\n",
    "    return text\n",
    "\n",
    "all_docs_text = \"\"\n",
    "pdf_folder_path = \"./data/\"\n",
    "\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder_path, \"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files to process.\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    print(f\"Processing: {os.path.basename(pdf_path)}\")\n",
    "    try:\n",
    "        doc_images = convert_from_path(pdf_path)\n",
    "        for page_data in doc_images:\n",
    "            all_docs_text += pytesseract.image_to_string(page_data) + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"      Error processing {os.path.basename(pdf_path)}: {e}\")\n",
    "\n",
    "print(\"\\nCleaning the combined text from all documents...\")\n",
    "cleaned_text = clean_text(all_docs_text)\n",
    "\n",
    "print(\"Splitting the combined text into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_text(cleaned_text)\n",
    "\n",
    "# 6. Save the processed chunks to the file ONCE\n",
    "with open('processed_chunks.txt', 'w') as f:\n",
    "    for chunk in chunks:\n",
    "        f.write(chunk + '\\n')\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(pdf_files)} PDFs and saved {len(chunks)} chunks to processed_chunks.txt\")"
   ],
   "id": "364018b7ca149995",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 PDF files to process.\n",
      "Processing: 8.pdf\n",
      "Processing: 6.pdf\n",
      "Processing: 7.pdf\n",
      "Processing: 5.pdf\n",
      "Processing: 4.pdf\n",
      "Processing: 1.pdf\n",
      "Processing: 3.pdf\n",
      "Processing: 2.pdf\n",
      "\n",
      "Cleaning the combined text from all documents...\n",
      "Splitting the combined text into chunks...\n",
      "\n",
      "Successfully processed 8 PDFs and saved 96 chunks to processed_chunks.txt\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T05:30:34.463691Z",
     "start_time": "2025-09-10T05:30:32.267647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    # eval_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    ")"
   ],
   "id": "743e1e3300c96346",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pragunkathuria/Downloads/evsLM-main/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T05:31:11.214076Z",
     "start_time": "2025-09-10T05:31:08.676147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ],
   "id": "77dcbe2a0043d793",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T05:31:14.023157Z",
     "start_time": "2025-09-10T05:31:13.671459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = chunks\n",
    "\n",
    "max_len = max([len(i) for i in data])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples, padding='max_length', truncation=True, max_length=max_len)\n",
    "\n",
    "\n",
    "data = [tokenize(i) for i in data]\n"
   ],
   "id": "6ff3b77f005bfd86",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
